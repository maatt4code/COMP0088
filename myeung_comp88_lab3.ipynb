{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maatt4code/COMP0088/blob/main/myeung_comp88_lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMP0088 Lab Assignment 3\n",
        "\n"
      ],
      "metadata": {
        "id": "iZgl8O7b-fi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this week's lab assignment we look at some non-parametric classification models and ensembles. These models have losses that are not smooth and aren't suited to simple gradient-based optimisation. You are only asked to produce naïve, brute force implementations of these models, but do note that such implementations can scale really badly, so be cautious about running the algorithms with large sample sizes. You may find it interesting to think about what strategies could be used to speed things up.\n",
        "\n",
        "Examples of the kinds of plots that will be produced by your finished code are shown below. Plotting code is provided, so your plots should look pretty similar, but the default resolution is lower to avoid the code taking too long to run.\n",
        "\n",
        "![example of completed plots](https://comp0088.github.io/assets/colab/week_3_small.jpg)\n",
        "\n",
        "The notebook uses a synthetic dataset (`week_3_data.csv`), which will be downloaded from the module GitHub in the **Setting Up** section below. This includes both binary and 3-class labels for the same data points, as illustrated in the following plots.\n",
        "\n",
        "![plots of data with binary and three-class labels](https://comp0088.github.io/assets/colab/week_3_data_small.jpg)"
      ],
      "metadata": {
        "id": "N12ZHil1_ZVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up"
      ],
      "metadata": {
        "id": "hxzyJ3xeT4LB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, this notebook makes use of the NumPy library for numerical computing and the Matplotlib library for plotting, so we need to import them. We will also use the [Pandas](https://pandas.pydata.org/docs/user_guide/index.html) library for loading data."
      ],
      "metadata": {
        "id": "4vHvSz5pReci"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gL8UJ7lgLznk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# this is probably the default, but just in case\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also fetch some shared COMP0088 lab code and data from the module GitHub:"
      ],
      "metadata": {
        "id": "K1RLN5QATflG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load lab code and resources\n",
        "!git clone https://github.com/comp0088/shared.git comp0088\n",
        "\n",
        "# at the moment this is all we care about\n",
        "import comp0088.utils as utils"
      ],
      "metadata": {
        "id": "v3X7LDC5KAob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c17a97da-a4fa-4bb0-e276-48423472df37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'comp0088'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "Receiving objects: 100% (14/14), 11.54 KiB | 11.54 MiB/s, done.\n",
            "remote: Total 14 (delta 3), reused 11 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up some items for later use. We just load the whole dataset here — later on we'll access particular subsets of it."
      ],
      "metadata": {
        "id": "t1rjWDV14cGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finally, set up some items for use in later code\n",
        "shared_rng = numpy.random.default_rng()\n",
        "\n",
        "# load the synthetic data\n",
        "df = pd.read_csv('comp0088/week_3_data.csv')"
      ],
      "metadata": {
        "id": "PfZQlfuELVwe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: $k$-Nearest Neighbours\n",
        "\n",
        "In a $k$-Nearest Neighbours classifier, test samples are compared directly against the samples in the training set and the $k$ most similar (according to some chosen similarity or distance metric) vote on the class to predict. Training such a classifier consists simply of memorising the training set. For the implementation below, we will skip this step and just pass the training data directly to the prediction function.\n",
        "\n",
        "Although other distance metrics may be useful for real problem classes, for the purposes of this exercise you can stick to a simple Euclidean distance. The NumPy function [`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html) is one way to calculate this.\n",
        "\n",
        "You may find the function `vote` in the `utils` module useful."
      ],
      "metadata": {
        "id": "FXpzvXtJAr4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Implement $k$-Nearest Neighbours prediction\n",
        "\n",
        "Implement the body of the `nearest_neighbours_predict` function in the cell below.\n"
      ],
      "metadata": {
        "id": "RGJxos1yA34M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_neighbours_predict ( train_X, train_y, test_X, neighbours=1 ):\n",
        "    \"\"\"\n",
        "    Predict labels for test data based on neighbourhood in\n",
        "    training set.\n",
        "\n",
        "    # Arguments:\n",
        "        train_X: an array of sample data for training, where rows\n",
        "            are samples and columns are features.\n",
        "        train_y: vector of class labels corresponding to the training\n",
        "            samples, must be same length as number of rows in X\n",
        "        test_X: an array of sample data to generate predictions for,\n",
        "            in same layout as train_X.\n",
        "        neighbours: how many neighbours to canvass at each test point\n",
        "\n",
        "    # Returns\n",
        "        test_y: predicted labels for the samples in test_X\n",
        "    \"\"\"\n",
        "    assert(train_X.shape[0] == train_y.shape[0])\n",
        "    assert(train_X.shape[1] == test_X.shape[1])\n",
        "    print(f\"train_X.shape: {train_X.shape}\")\n",
        "    print(f\"train_y.shape: {train_y.shape}\")\n",
        "\n",
        "    # TODO: implement the k-nearest neighbours algorithm\n",
        "    # These are the labels of our training data after training\n",
        "    test_y = np.zeros(test_X.shape[0])\n",
        "    for i, test_x in enumerate(test_X):\n",
        "      dist = []\n",
        "      for j, train_x in enumerate(train_X):\n",
        "        # Euclidean Distamce of test data wrt ALL training data\n",
        "        #dx = np.sqrt(np.sum((x - train_X) ** 2))\n",
        "        dx = np.linalg.norm(test_x - train_x)\n",
        "        # Keep track of distance and labels\n",
        "        dist.append((dx, train_y[j]))\n",
        "\n",
        "      dist.sort(key=lambda z: z[0])\n",
        "      candidates = [label for _, label in dist[:neighbours]]\n",
        "      # the vote function just returns whichever occurs most often in the list\n",
        "      test_y[i] = utils.vote(candidates)\n",
        "\n",
        "\n",
        "    return test_y"
      ],
      "metadata": {
        "id": "tbnDPv-S6pHo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 1\n",
        "\n",
        "Execute the code cell below to run your k-NN function on the test data and plot the results.\n",
        "\n",
        "Feel free to try out different values for the configuration variables `NUM_SAMPLES`, `RESOLUTION` and `NEIGHBOURS` -- but note that larger values can slow things down significantly."
      ],
      "metadata": {
        "id": "NPwNjtpYh_F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES = 50\n",
        "RESOLUTION = 100\n",
        "NEIGHBOURS = 3\n",
        "\n",
        "X = df[['X1','X2']].values[:NUM_SAMPLES,:]\n",
        "y = df['Multi'].values[:NUM_SAMPLES]\n",
        "\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "ax = fig.subplots()\n",
        "\n",
        "# this just tests to see whether there's a functioning implementation\n",
        "# before attempting to pass it to the plotting utility\n",
        "dummy = nearest_neighbours_predict ( X[:2,:], y[:2], X[:2,:], neighbours=NEIGHBOURS )\n",
        "if dummy is None:\n",
        "    utils.plot_unimplemented(ax, f'{NEIGHBOURS}-Nearest Neighbours')\n",
        "else:\n",
        "    nn_cls = lambda z: nearest_neighbours_predict ( X, y, z, neighbours=NEIGHBOURS )\n",
        "    utils.plot_classification_map(ax, nn_cls, X, y, resolution=RESOLUTION, title=f'{NEIGHBOURS}-Nearest Neighbours')\n"
      ],
      "metadata": {
        "id": "m5XyX-uSFWON",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "outputId": "d7c789b9-02fd-4ead-c429-a82be3d74384"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X.shape: (2, 2)\n",
            "train_y.shape: (2,)\n",
            "train_X.shape: (50, 2)\n",
            "train_y.shape: (50,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhQAAAIkCAYAAABGALJIAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUqZJREFUeJzt3Xl4VPW9P/D3OWf2yUxISAJJCGFJIkU2AUFBhT4Gwa0XrhbL1VawUq3QC6VqxVqR1hbby7XeUqto69LNaqn7gkIVvfyEakFsQQMJlzUJWYBkMpNklnO+vz/CjFlhklnOLO/X88xD58yZmU9GyrzzOZ/zPZIQQoCIiIgoArLeBRAREVHyY6AgIiKiiDFQEBERUcQYKIiIiChiDBREREQUMQYKIiIiihgDBREREUWMgYKIiIgixkBBREREEWOgIKKUJEkSHnjggQE/d/ny5efc74EHHoAkSWhsbBzQ+xClEgYKonPYt28fvvrVr2LUqFGw2WzIycnBZZddhtdeey3s1xgxYgQkScJ3vvOdHo9t27YNkiRh06ZN0SxbF2+++Wa/vsRnz54NSZJw7bXX9njs8OHDkCQJ69evj2KFRBQrDBRE53DkyBG0tLTg5ptvxv/8z//ghz/8IQDgK1/5Cp544ol+vdaTTz6JmpqaWJSZEN58802sXbu23897/fXXsWvXrqjW0tbWhvvuuy+qr0lEfWOgIDqHq666Cps3b8aaNWuwdOlSrFixAu+99x4mTpyIhx9+OOzXOf/886GqKh566KEYVhu+QCAAn8+ndxkYPnw4srKyBhREzsZiscBgMET1NROdx+PRuwRKYwwURAOgKAqKiorQ1NQU9nNGjBiBb3zjG2F3Kaqrq3HLLbdgyJAhMJvNOP/88/HUU0912cfn8+H+++/HlClTkJmZCbvdjksvvRTvvfdel/06Hz545JFHMHr0aJjNZnz22WcAgIqKClx//fXIzs6GxWLB1KlT8eqrr3Z5Db/fj7Vr16K0tBQWiwWDBw/GJZdcgi1btgAAFi9ejEcffRRAxwxC8HYuDocD3/3ud/Haa69h9+7d59y/qakJK1euRFFREcxmM0pKSvCzn/0MmqZ12a+3GYpt27Zh6tSpsFgsGD16NDZu3Biag+jNyy+/jHHjxoU+/82bN/e6X2NjIxYuXAin04nBgwdjxYoVaG9v77JPIBDAj3/849BnP2LECNx7773wer3nrBvo+PuzePHi0P1nnnkGkiTh/fffxx133IG8vDwMGzYMANDS0oKVK1dixIgRMJvNyMvLw5w5c8L6fIkGKr3iO1EEPB4P2tra0NzcjFdffRVvvfUWbrjhhn69xg9+8AP87ne/w0MPPYRf/vKXfe5XV1eHiy66KDQcmJubi7feegvf/OY34XK5sHLlSgCAy+XCb37zGyxatAhLly5FS0sLfvvb32Lu3Ln46KOPMGnSpC6v+/TTT6O9vR3f+ta3YDabkZ2djX379mHmzJkoLCzEPffcA7vdjhdeeAHz58/HX//6VyxYsABAxwDiunXrcOutt2LatGlwuVz4xz/+gd27d2POnDm47bbbUFNTgy1btuD3v/99vz6XFStW4Be/+AUeeOCBHkGms9bWVsyaNQvV1dW47bbbMHz4cHz44YdYvXo1amtr8cgjj/T53E8++QTz5s1Dfn4+1q5dC1VV8aMf/Qi5ubm97r99+3a8+OKLuOOOO+BwOPDLX/4S1113HY4ePYrBgwd32XfhwoUYMWIE1q1bh507d+KXv/wlTp8+jd/97nehfW699VY8++yzuP766/G9730Pf//737Fu3Tp8/vnneOmll/r1eXV2xx13IDc3F/fff3+oQ3H77bdj06ZNWL58OcaOHYuTJ09i+/bt+PzzzzF58uQBvxfRWQkiCsttt90mAAgAQpZlcf3114tTp06F9dzi4mJx9dVXCyGEWLJkibBYLKKmpkYIIcR7770nAIi//OUvof2/+c1vivz8fNHY2Njldb72ta+JzMxM0draKoQQIhAICK/X22Wf06dPiyFDhohbbrkltO3QoUMCgHA6naK+vr7L/pdffrkYP368aG9vD23TNE3MmDFDlJaWhrZNnDgx9DP0ZdmyZaI//6zMmjVLnH/++UIIIdauXSsAiF27dnWp+b/+679C+//4xz8WdrtdHDhwoMvr3HPPPUJRFHH06NHQNgBizZo1ofvXXnutsNlsorq6OrStsrJSGAyGHjUDECaTSVRVVYW2ffrppwKA2LBhQ2jbmjVrBADxla98pcvz77jjDgFAfPrpp0IIIfbs2SMAiFtvvbXLfnfeeacAIN59990+6w4qLi4WN998c+j+008/LQCISy65RAQCgS77ZmZmimXLlvV4DaJY4iEPojCtXLkSW7ZswbPPPosrr7wSqqoOaAbhvvvuQyAQ6HOWQgiBv/71r7j22mshhEBjY2PoNnfuXDQ3N4da14qiwGQyAQA0TcOpU6cQCAQwderUXtvb1113XZffyE+dOoV3330XCxcuREtLS+h9Tp48iblz56KyshLV1dUAgEGDBmHfvn2orKzs988cjhUrVpxzluIvf/kLLr30UmRlZXX5XMrLy6GqKj744INen6eqKrZu3Yr58+ejoKAgtL2kpARXXnllr88pLy/H6NGjQ/cnTJgAp9OJ//u//+ux77Jly7rcD57N8+abb3b5c9WqVV32+973vgcAeOONN/r8mc9l6dKlUBSly7ZBgwbh73//e0oPAFPiYaAgCtOYMWNQXl6Ob3zjG3j99dfhdrtDX/oA0NzcjBMnToRup06d6vV1Ro0aha9//et44oknUFtb2+PxhoYGNDU14YknnkBubm6X25IlSwAA9fX1of2fffZZTJgwITTXkJubizfeeAPNzc09XnvkyJFd7ldVVUEIgR/+8Ic93mvNmjVd3utHP/oRmpqaUFZWhvHjx+Ouu+7CP//5zwF8kr3LzMzEypUr8eqrr+KTTz7pdZ/Kykps3ry5R63l5eVdau2uvr4ebW1tKCkp6fFYb9uAjmHR7rKysnD69Oke20tLS7vcHz16NGRZxuHDhwF0nCkky3KP9xo6dCgGDRqEI0eO9FpDOLr/NwWAn//859i7dy+Kioowbdo0PPDAA70GIaJo4gwF0QBdf/31uO2223DgwAGcd955WLFiBZ599tnQ47NmzcK2bdt6fe4PfvAD/P73v8fPfvYzzJ8/v8tjweHCm266CTfffHOvz58wYQIA4A9/+AMWL16M+fPn46677kJeXh4URcG6detw8ODBHs+zWq29vtedd96JuXPn9vpewS/Byy67DAcPHsQrr7yCd955B7/5zW/wi1/8Ao8//jhuvfXWXp/bX8FZirVr1/Y6D6FpGubMmYO777671+eXlZVFpQ4APX7rDwoGyLPpa8gznCHVvqiq2uv27v9NgY6ZjksvvRQvvfQS3nnnHfzXf/0Xfvazn+HFF1/ssyNDFCkGCqIBamtrA4BQJ+Duu+/GTTfdFHo8Kyurz+eOHj0aN910EzZu3Ijp06d3eSw3NxcOhwOqqoZ+8+7Lpk2bMGrUKLz44otdvqyC3YVzGTVqFADAaDSe870AIDs7G0uWLMGSJUvgdrtx2WWX4YEHHggFiki+MIEvuhQPPPBAr2Fq9OjRcLvdYdXaWV5eHiwWC6qqqno81tu2/qqsrOzSKaiqqoKmaRgxYgQAoLi4GJqmobKyEl/60pdC+9XV1aGpqQnFxcWhbVlZWT3OHvL5fL12s84mPz8fd9xxB+644w7U19dj8uTJ+MlPfsJAQTHDQx5E59BbG93v9+N3v/sdrFYrxo4dCwAYO3YsysvLQ7cpU6ac9XXvu+8++P1+/PznP++yXVEUXHfddfjrX/+KvXv39nheQ0NDl32Brr81//3vf8eOHTvC+tny8vIwe/ZsbNy4sc/DL0EnT57s8lhGRgZKSkq6nPZot9sBoF+n03a3cuVKDBo0CD/60Y96PLZw4ULs2LEDb7/9do/HmpqaEAgEen1NRVFQXl6Ol19+uctcQVVVFd56660B1xoUPF02aMOGDQAQ+vK+6qqrAKBH1yW4jsnVV18d2jZ69OgesyBPPPFEnx2K7lRV7XG4Ky8vDwUFBT1OUSWKJnYoiM7htttug8vlwmWXXYbCwkKcOHECf/zjH1FRUYH//u//RkZGxoBeN9il6HyYJOihhx7Ce++9h+nTp2Pp0qUYO3YsTp06hd27d2Pr1q2h+YxrrrkGL774IhYsWICrr74ahw4dwuOPP46xY8fC7XaHVcejjz6KSy65BOPHj8fSpUsxatQo1NXVYceOHTh+/Dg+/fRTAB2Bafbs2ZgyZQqys7Pxj3/8I3RqYlAwRP3nf/4n5s6dC0VR8LWvfa1fn0tmZiZWrFjR63DmXXfdhVdffRXXXHMNFi9ejClTpsDj8eBf//oXNm3ahMOHDyMnJ6fX133ggQfwzjvvYObMmfj2t78NVVXxq1/9CuPGjcOePXv6VWN3hw4dwle+8hXMmzcPO3bswB/+8Af8x3/8ByZOnAgAmDhxIm6++WY88cQTaGpqwqxZs/DRRx/h2Wefxfz58/HlL3859Fq33norbr/9dlx33XWYM2cOPv30U7z99tt9/lzdtbS0YNiwYbj++usxceJEZGRkYOvWrfj444/x3//93xH9nERnpecpJkTJ4LnnnhPl5eViyJAhwmAwiKysLFFeXi5eeeWVsF+j82mjnVVWVgpFUXqcNiqEEHV1dWLZsmWiqKhIGI1GMXToUHH55ZeLJ554IrSPpmnipz/9qSguLhZms1lccMEF4vXXXxc333yzKC4uDu3X2ymYnR08eFB84xvfEEOHDhVGo1EUFhaKa665RmzatCm0z4MPPiimTZsmBg0aJKxWqxgzZoz4yU9+Inw+X2ifQCAgvvOd74jc3FwhSdI5TyHtfNpoZ6dPnxaZmZm91tzS0iJWr14tSkpKhMlkEjk5OWLGjBli/fr1XWpBL6df/u1vfxMXXHCBMJlMYvTo0eI3v/mN+N73vicsFkuX/QD0etpl91M3g6eNfvbZZ+L6668XDodDZGVlieXLl4u2trYuz/X7/WLt2rVi5MiRwmg0iqKiIrF69eoup+sKIYSqquL73/++yMnJETabTcydO1dUVVX1edroxx9/3OX5Xq9X3HXXXWLixInC4XAIu90uJk6cKH7961/3+HmIokkSIowJIyKiFDV//vyYng5LlC44Q0FEaSM4SBtUWVmJN998E7Nnz9anIKIUwg4FEaWN/Px8LF68GKNGjcKRI0fw2GOPwev14pNPPumxlgQR9Q+HMokobcybNw/PPfccTpw4AbPZjIsvvhg//elPGSaIooAdCiIiIooYZyiIiIgoYgwUREREFLG0mqHQNA01NTVwOBwRLxFMRESU6oQQaGlpQUFBAWT57D2ItAoUNTU1KCoq0rsMIiKipHLs2DEMGzbsrPukVaBwOBwAgKeOPQWb06ZzNURERImt1dWKW4puCX1/nk1aBYrgYQ6b08ZAQUREFKZwxgQ4lElEREQRY6AgIiKiiDFQEBERUcTSaoYiXJIqQfbLkJBap5YKCGhGDULh4qhERBRdDBSdCSDjRAYymjIgp2jzRoMG9yA33EPdSLG8REREOmKg6CTjRAYGNQ1CTl4OTDZTyi1+JYSAr9WHxvpGAIA7361zRURElCoYKM6QVAkZTRnIycuBY/C5z7dNVmarGQAQqA/Ak+fh4Q8iIoqK1OzrD4DslyFDhslm0qcAAfgCPiAO3+8mmwkyZMh+/ucnIqLo4DfKGcEBTL0Oc/gCPpx2n+4IFTEW/BlTbeiUiIj0w0CRINr97dA0DV6/V+9SiIiI+o0zFHoRQJuvDZrQAADtvnYIIdDmawtd0U2WZFhNVp6NQURECY8dCp0ICLjb3XC1uuBqdUHVVCiyAlVTQ9vc7W6IMIcqnnz0SUwYMQFDLUNRPr0cuz7aFeOfgIj6QwiB6pPVEIKD0JSaGCiiTNMEPjvuxocHTuOz425oWu//eEiShGxHNixGS+h+8AYAFqMF2Y7ssGY6Xnz+Rdy36j58f833sW33NoybOA7Xzb0ODfUN0fvBiCgiRxuOYtP2TTjacFTvUohigoc8ouijqiY89f5xHK5vg18VMCoSRuRZccusYZhWMqjH/oqsYFDGIDQ0N0DVVGhCg4AIbQ93QPTXD/8a31j6Ddy45EYAwMOPP4x33ngHf3jqD/juPd+N5o9IRANUVVuFlrYWHKw9iOK8Yr3LIYo6Booo+aiqCQ++dBAer4pMmwEmRYJPFaisbcWDLx3EfQtG9xoqAmoAmqYBApBkCUIT0DQNATUAo8F4zvf1+XzYs2sPvrv6i+AgyzJmlc/Cxzs+juaPSET9oAkNe4/shc/fcebW/uP7EVADqDheAafNCQAwGU0YVzwOssRmMSU/Booo0DSBp94/Do9XRZ7T+MVhC1mC2WlEvcuPp94/jqmjMiHLXbsOXr8XkiTBarYiw5IBd7sb7b52eP3esALFycaTUFUVuUNyu2zPHZKLyorK6P2QRNQvATWAnRU70ehqhKZpkCQJTpsTp92nsXnXZsiyjBxnDsYMGwOTQaf1b4iiiLE4CipqPDhc34ZMm6HHYQpJkpBpM+BwfRsqajw9nms2mpFpz0SmLROKoiDTlolMeybMRnO8yieiGDAZTFg0axHKCsqgyArMRnPopsgKygrKsGjWIoYJShkMFFHQ1OqHXxUwKb3PPJgUCQFNoKnV3+Mxo8EIi8nyxamhEmAxWcLqTgDA4JzBUBQFDXVdBzAb6hqQNzSvXz8HEUWXw+rAghkL4LQ54fP74G53w+f3wWlzYsGMBXBYU3eZf0o/DBRRMMhmhPHMzERvfKqAQZYwyBZeSOgPk8mESVMm4f2/vR/apmkaPvjbB7jw4guj/n5E1D91TXVwt7uhaipMigmqpsLT7kF9U73epRFFFQNFFIwpsGNEnhXNrYEe55gLIdDcGsCIPCvGFNhj8v53rLoDv3vyd3ju2eew//P9WPXtVfB4PKGzPohIP4frDsMgGzClZApuueIWTC6ZDEVWcKjukN6lEUUVhzKjQJYl3DJrGB586SDqXf4uZ3k0twZgNyu4ZdawHgOZ0fLvN/w7Ghsa8dP7f4r6E/UYP2k8Nm3ehLwhPORBpLeRQ0Yix5mD0oJSSJKEq6ZehdKCUh7uoIQU2Dmt631PS9jPZaCIkmklg3DfgtGhdShcWsdhjtJ8W5/rUETTt5Z/C99a/q2YvgcR9V9+dj7ykR+6L0kSygrLdKyIKDYYKKJoWskgTB2ViYoaD5pa/RhkM2JMgT1mnQkiIqJEwUARZbIsYeywDL3LICIiiisOZRIREVHE2KEgIiJKA90HLqONHQoiIiKKGAMFERERRYyBgoiIiCLGGQoiIiKdaRpwvAbwtAJ2GzCsAJD78St/rOcjwsFAQUREpKP9VcDW9yXU1QMBFTAowJA8oHyWwHklelcXPh7yICIi0sn+KuD5lyRU1wImM+B0dPxZXduxfX+V3hWGj4EiBfy/D/4fvnbt1/Clgi8hS8rCGy+/oXdJRER0DprW0Zlo9wKZTsBk7DjMYTJ23G/3djyuaXpXGh4e8oiGgA+GIx8Bqq/vfRQTAsXTAIMp6m/f6mnFuInjcNMtN+Hr//71qL8+JR4hBGpO1aAguwCSxKXdiZLR8Rqgrh6w2YDu/zeWpI7tdfUd+w0fpk+N/cFAEQVK/X5Y37gPktfd5z7CnIHW6zdALRgf9fefc+UczLlyTtRflxLX0YajeHnHy5h/8XwU5xXrXQ4RDYCntWNmwqb0/rhBAdo0wLXrPASOZ8a3uAHgIY8oUIeOhTZ4BCABmiOvxw0SoA0eAXXoWL1LpRRRVVuFlrYWHKw9qHcpRDRAdltHaAiovT8eUAFFBuzm5PjdPzmqTHSyAu+F34DttdUdhz0MFggISJAAfzsgdTwOuZcYKgCf6oNJMQHsXFMfNKFh75G98Pk7DqvtP74fATWAiuMVcNqcAACT0YRxxeMgS/w9gSgZDCvoOJujuhYwOrse9hACaG0FCvOBwkybfkX2AwNFlARKZkEd+iUotXuhZgyBqgWgyAYo7c1Q889HoGRWr8/zBXw47TmNLHsWTMboz1dQagioAeys2IlGVyM0TYMkSXDanDjtPo3NuzZDlmXkOHMwZtgYmGIwp0NE0SfLHaeGPv+ShGZXx8xEsGPR2gpYzB2Py43J8dsmf5WJljNdCkgKhL8NAoDwtwGS3Hd3AkC7vx2apsHr98a3XkoqJoMJi2YtQllBGRRZgdloDt0UWUFZQRkWzVrEMEGUZM4rAW5YIFCYD/i8QIu748/C/I7tybQORdJ2KB566CGsXr0aK1aswCOPPKJvMQJo87VBGzYVhtwyGE7sg5qRC7m9Cb4h56N52FTI3jZYTVYAZ/YVHecBtfvaIYRAm68N8pll0WRJ7tg3OUIpxYnD6sCCGQvw+JuPo9nTDFVToaoqMu2ZWDBjAZQ+QisRJabg6pajAYwcK1Bd2AqPNwC72YDCTBvkRgmBRn1r7I+kDBQff/wxNm7ciAkTJuhdCgBAQMDd7kZADUAdNx/5dZ9D8bZAQEb9uPnwtHtgUAywmCwAENo3SJEVqJoKV6sLAEL7SmEmCrfbjUNVh0L3jxw6gn/t+RcGZQ9C0fCiKP6kpLe6pjq4291QNRVWkxVuvxuedg/qm+qRn52vd3lENECyJKFokF3vMiKSdIc83G43brzxRjz55JPIysrSuxwAgCRJyHZkw2K0wDN8Ory5pVBaT8GbWwrP8OmwGC3IdmRDkqQu+wafG7wB6LJvuPb8Yw8uu+AyXHbBZQCAH6z6AS674DKsu39d9H9Y0tXhusMwyAZMKZmCW664BZNLJkORFRyqO3TuJycZIQSqT1ZDCKF3KUQUhqTrUCxbtgxXX301ysvL8eCDD+pdTogiKxiUMQgNqh+nJlyPIa5anJpwPRSDCYMyBnUJCKF9mxugaio0oUFAhLb3d6GiS2ZfgtPidLR/JEpAI4eMRI4zB6UFpZAkCVdNvQqlBaVwWB16lxZ1XGuDKLkkVaD485//jN27d+Pjjz8Oa3+v1wuv94thR5fLFavSAHRM4muaBs/wi1FzVQHaBxVD0jQE1ACMBmOv+0IAkixBaAJaH/sSBeVn5yMfXxzakCQJZYVlOlYUO53X2mCgoGSTCFf/jLekCRTHjh3DihUrsGXLFlgslrCes27dOqxduzbGlX3B6/dCkiRYrXZkZE2D3O5Gu68dXr+3R0gI7Wu2IsOSAXe7G22+NrR6W5GpZHIgk9JOuGttnD/8fJw4fYLLjhMlmKQJFLt27UJ9fT0mT54c2qaqKj744AP86le/gtfrhaJ0nXJfvXo1Vq1aFbrvcrlQVBS7IUWz0dwxUGm0ABKQacsMndYXzr6KrMDT7oHVZOWaFJR2wl1rw2a24Y2P3uChEKIEkzSB4vLLL8e//vWvLtuWLFmCMWPG4Pvf/36PMAEAZrMZZrM5XiXCaDDCiE6dCAmhMzvC2VcTGjTRsSYFAwWlm+BaG2/vehuVNZUwGowwG83QhAZ/wI/SglLMnTIXHx34iIdCiBJQ0gQKh8OBcePGddlmt9sxePDgHtuThuCaFESd9bbWRiAQgMlowvC84dh/fD+XHSdKUEkTKFJR5/UrgiJdk4Io2XVfa8Pr88LV6sI7u98BAC47TrpLx4HLcCR1oNi2bZveJUQkuCaFy+NCu7/9izUpIEEIAYvRAqfdycEzSivBtTbGl4zHzLEzsf2z7fjsyGewW+1o9jT3eSiEYYJIX0kdKFJBtNekIEp2fa21YTPb8MrOV7jsOFGC4gHHBNBlTQpJAgRCa1IQpZv87HyUFZaFwnRwrQ1ZlkOHQkyKCaqmhpYdJyL9sUORAHpbk6Kv9SuI0lVvh0IqjlXgUN0hXseEoobzEQPHQJEA+rN+RXcPr3sYr7/4OiorKmGxWjBtxjQ88LMHUHpeaRwqJ4qfdFp2nCgZ8ZBHDAghsP/k/rAvamQ0GDvWqwiOS5xZvyKc7sSH73+IW5fdind2voMXt7wIv9+Pf7/i3+HxeCL4CYgST1+HQtidIEoM7FDEwN6GvVi/Yz3uvPhOjM8bH9P32rR5U5f7v37m1yjNK8WeXXsw87KZMX1vIiK9CSFQc6qGS7EnAHYoYuAftf/AybaT2FW7K+7v7WruWL8iKzsxLu1ORBRLRxuOYtP2TTjacFTvUtIeOxRRoAkN245sQ5u/DQCw4/gO+FQfPjz+IXJtuQAAq9GK2cWzY7qSn6ZpWL1yNabPnI6x48bG7H2IiBJFuFel5bBl7DFQRIFP9eHFihdx3HUcAS0AWZIx2DYYJ9wn8Niux2CQDRjmHIYZw2bAYgjvSqkDceeyO/H53s/x1va3YvYeRER6CveqtFyKPf4YKKLAYrBg7ay12LhrIz6q+QgWgwV2ox1CCLQH2jGtYBpun3J7TMPEXcvvwtuvv403P3gThcMKY/Y+RER6CveqtFyKPf4Y36JksHUw7p5xN3JtuWjzt+F0+2m0+duQa8vF3TPuRrY1OybvK4TAXcvvwhsvvYFX330VxSN59UUiSl3Bq9KWFZRBkRWYjebQTZEVlBWUYdGsRQwTOmCgiKJDTYdwuv00AloAFsWCgBbA6fbTONx0OGbveeeyO/HCH17Ak396EhmODNSdqEPdiTq0tbXF7D2JiPQUvCqt0+aEz++Du90Nn98Hp82JBTMWcG0SnfCQRxR9WvcpDLIBV5ZciYVjF+KFz17A9mPbsaduD0qyS2Lynk899hQA4JrZ13TZ/ujTj+I/Fv9HTN6TiEhPgZ3TUNteBXezH6omwWJwwhdogjvgR+0HeRhqGa13iWmJgSKKJg2ZhCJnEaYVTIMkSVg2dRkuLLgQg62DY/aep8XpmL02EVGiOtL2LyiSARdkzsPFWf+OD0//FfvdH+Jw2z8ZKHTCQBFFJdklKMEXnQhJkjC9cLqOFRERpaYR1gkYbCpEie1CSJKEebm3ocQ+GQ4ldr/A0dkxUBARUdIZahmNofiiEyFJEkrtXGtCTwwURESUELj4VHLjWR5EREQUMQaKMwQ6rgwa7hVCk1nwZwz+zERERJFioDhDM2rQoMHX6tO7lJjztfqgQYNm1PQuhYiIUgRnKM4QioB7kBuN9Y0AAJPNlHKXwhVCwNfqQ2N9I9yD3BAKOxRERBQdDBSduIe6AQCB+gDkFG3eaNDgHuQO/axERPHAgcvUx0DRmQS4893w5Hkg+2VISLEOBQQ0o8bOBBERRR0DRS+EIqAqqt5lEBERJY3U7OsTERFRXDFQEKUhIQSqT1anxWnSRBQfPORBlIaONhzFyztexvyL56M4r1jvcijFcAAzPbFDQZSGqmqr0NLWgoO1B/UuJe7YnSGKDXYoiNKAJjTsPbIXPn/Hwm37j+9HQA2g4ngFnDYnAMBkNGFc8TjIUmr/nsHuDFFsMFAQpYGAGsDOip1odDVC0zRIkgSnzYnT7tPYvGszZFlGjjMHY4aNgclg0rvcmOrcnWGgIIoeBgqiNGAymLBo1iK8vettVNZUwmgwwmw0QxMa/AE/SgtKMXfK3JQME+zOEMUHAwVRmnBYHVgwYwEef/NxNHuaoWoqVFVFpj0TC2YsgCIrYb2OEAI1p2pQkF2QFMvTsztDFB+M40RppK6pDu52N1RNhUkxQdVUeNo9qG+qD/s1jjYcxabtm3C04WgMK42eYHemrKAMiqzAbDSHboqsoKygDItmLWKYIIoQAwVRGjlcdxgG2YApJVNwyxW3YHLJZCiygkN1h8J+jWQ8QyTYnXHanPD5fXC3u+Hz++C0ObFgxgI4rA69SyRKejzkQZRGRg4ZiRxnDkoLSiFJEq6aehVKC0rP+oWaKjMInbszVpMVbr871J3Jz87XuzyipMdAQZRG8rPzkY8vvjwlSUJZYdlZn5MqMwjB7sz4kvGYOXYmtn+2HRXHKnCo7hADRYS4kBUBDBREdA6pcobIQLozRBS+xO1PElHCSIUZhPzsfJQVloXOTAl2Z9idIIoOBgoiCks0zhAhotTFQx5EFBbOIBDAeQnqGwMFEYWFMwhEdDYMFEQUloGcIUJE6YMzFERERBQxBgoiIiKKGA95EBFRnziESeFih4KIiIgixkBBREREEWOgICIioogxUBAREVHEOJRJRJSgwh2INFz0UYwrITo3diiIiIgoYgwUREREFDEGCiIiIooYZyiIiBKE3otI6f3+lNzYoSAiIqKIMVAQERFRxBgoiIiIKGIMFERERBQxDmUSEekgmgOQ3V8rnIWuOIBJ0cYOBREREUWMgYKIiIgixkBBREREEWOgICIioohxKJOIKMbiPQDJgUvSAzsUREREFDEGCiIiIooYAwURERFFjDMURERRxhkGSkfsUBAREVHEGCiIiIgoYgwUREREFDEGCiIiIooYAwURERFFjIGCiIiIIsZAQURERBFjoCAiIqKIcWErIqIIcBErog7sUBAREVHEGCiIiIgoYgwUREREFDEGCiIiIopYUgWKdevW4cILL4TD4UBeXh7mz5+P/fv3610WERFR2kuqQPH+++9j2bJl2LlzJ7Zs2QK/348rrrgCHo9H79KIiIjSWlKdNrp58+Yu95955hnk5eVh165duOyyy3SqioiIiJKqQ9Fdc3MzACA7O1vnSoiIiNJbUnUoOtM0DStXrsTMmTMxbty4Xvfxer3wer2h+y6XK17lERERpZWkDRTLli3D3r17sX379j73WbduHdauXRvHqogo1XFlTKLeJeUhj+XLl+P111/He++9h2HDhvW53+rVq9Hc3By6HTt2LI5VEhERpY+k6lAIIfCd73wHL730ErZt24aRI0eedX+z2Qyz2Ryn6oiIiNJXUgWKZcuW4U9/+hNeeeUVOBwOnDhxAgCQmZkJq9Wqc3VERETpK6kOeTz22GNobm7G7NmzkZ+fH7o9//zzepdGRESU1pKqQyGE0LsEIiIi6kVSdSiIiIgoMTFQEBERUcQYKIiIiChiSTVDQUQUK1ywiigy7FCkMSEEqk9Wc9iViIgixkCRxo42HMWm7ZtwtOGo3qUQEVGSY6BIY1W1VWhpa8HB2oN6lxLCrgkRUXLiDEUa0YSGvUf2wuf3AQD2H9+PgBpAxfEKOG1OAIDJaMK44nGQJX2y5tGGo3h5x8uYf/F8FOcV61IDpQfOTBBFFwNFGgmoAeys2IlGVyM0TYMkSXDanDjtPo3NuzZDlmXkOHMwZtgYmAwmXWrs3DVhoCAiSh4MFGnEZDBh0axFeHvX26isqYTRYITZaIYmNPgDfpQWlGLulLlxDRPJ0DUhIqJzY6BIMw6rAwtmLMDjbz6OZk8zVE2FqqrItGdiwYwFUGQlrvUkQ9eEiIjOjb/ypaG6pjq4291QNRUmxQRVU+Fp96C+qT7utQS7JmUFZVBkBWajOXRTZAVlBWVYNGsRwwQRUYJjhyINHa47DINswPiS8Zg5dia2f7YdFccqcKjuEPKz8+NeT6J1TSj1dB/A1IRAdbMHHm8AdrMBhZk2yJKkU3Xpp+Pzb+Xnn2IYKNLQyCEjkePMQWlBKSRJwlVTr0JpQSkcVoduNXXumlhNVrj97lDXRI+QQ6mrssGFrQdqUe/2QtU0KLKMvAwzysvyUZrr1Lu8lMfPP3XxkEcays/OR1lhGaQzvxFIkoSywjJdv7iDXZMpJVNwyxW3YHLJZCiygkN1h3SriVJPZYMLL+w5glpXG8wGGQ6LEWaDjFpXG17YcwSVDS69S0xp/PxTGzsUlBASsWtCqUUTAlsP1MIbUOG0GEOBWlYkOC1GuNr92HqgFqNzHGy/xwA//9THQEEJIT87H/n4okMS7JoQRUt1cyvq3V7YTIbQl1mQJEmwmgyod3tR3dyKokF2napMXfz8Ux8PeRBRWvB4A2eO2ff+269BlqBqGjzeQJwrSw/8/FMfAwURpQW72QBFlqFqvV8nJqAJKLIMu5mN21jg55/6GCiIKC0UZtqQl2FGqy/Q4+JzQgi0+QLIyzCjMNOmU4WpjZ9/6mOgIKK0IEsSysvyYTYocLX74VM1aELAp2pwtfthNigoL8vnQGCM8PNPfewtEaWYcK6iabjoozhUEn2RXiG0NNeJhZOKQ+sgtJ1ZByHfaeU6CHEw0M+fC2ElBwYKIkorpblOjM5x8AtKJ/39/LkQVvJgoCCitCNLEk9N1FG4n39wISxvQIXNZIAiK1A1EVoIa+GkYoaKBMIZCiIiSjjdF8IyKjJkSYJRkeG0GOENqNh6oBaa6P2sEYo/BgoiIko4/VkIixIDD3kQJbGBDilGOtwYqXCHQvWuk/TzxUJYvV9t2CBLaONCWAmFHQoiIko4XAgr+TBQEBFRwuFCWMmHgYKIiBIOF8JKPgwURESUkIILYeU7rfAFNLS0++ELaMh3WnnKaALiwSciijsOW1K4uBBZ8mCgICKihMaFyJIDD3kQERFRxBgoiIiIKGIMFERERBQxBgoiIiKKGAMFERERRYyBgoiIiCLGQEFEREQRY6AgIiKiiDFQEBERUcQYKIiIiChiDBREREQUMQYKIiIiihgDBREREUWMgYKIiIgixkBBREREEWOgICIioogZ9C5AD1e+93c47eaIXuO18hlRqoaIiCj5sUNBREREEWOgICIioogxUBAREVHEGCiIiIgoYmk5lBkN1279MKz94jm8KYRAzakaFGQXQJKkuL0v6cdw0Uc9tgV2TtOhEiJKd+xQpJCjDUexafsmHG04qncpRESUZhgoUkhVbRVa2lpwsPag3qUQEVGa4SGPJKYJDXuP7IXP7wMA7D++HwE1gIrjFXDanAAAk9GEccXjIEvMjkREFDsMFDHWfdYimjMVATWAnRU70ehqhKZpkCQJTpsTp92nsXnXZsiyjBxnDsYMGwOTwRS196XE1n2ugjMVRBQP/LU1iZkMJiyatQhlBWVQZAVmozl0U2QFZQVlWDRrEcMEERHFHANFknNYHVgwYwGcNid8fh/c7W74/D44bU4smLEADqtD7xKJiCgNMFCkgLqmOrjb3VA1FSbFBFVT4Wn3oL6pXu/SiIgoTTBQpIDDdYdhkA2YUjIFt1xxCyaXTIYiKzhUd0jv0oiIKE1wKDPOelsQK9JBzZFDRiLHmYPSglJIkoSrpl6F0oJSHu4gAL0vftUbDm8SUSQYKFJAfnY+8pEfui9JEsoKy3SsiIiI0g0PeRAREVHEGCiIiIgoYgwUREREFDHOUBARAK6wSUSRYYeCiIiIIsZAQURERBHjIQ8iogQnaT7knPoYsvD3uY8mGdGYfSGEzGv3kD4YKBJALBa7IooUF8RKHJktBzDxs7UwBtx97uM3ZOAfEx9GU+a4OFZG9AUe8iCiECEEqk9WQwihdynUSZPzS/DYigEAbea8HjcBCR5bMZqcX9K5UkpnDBREFHK04Sg2bd+Eow1H9S6FOpMU/F/xjRCSAlnzA5IcusmaD5Bk/F/xjYCk6F0ppTEGCiIKqaqtQktbCw7WHtS7FOqmLudSNDvGwOxv6rLdFGhGs2MM6nIu1acwojM4Q0GUxjShYe+RvfD5fQCA/cf3I6AGUHG8Ak6bEwBgMpowrngcZIm/f+jqTJfigr0/hKx6oSlmyGo7AHYnKDEwUCSo7oOaiTKkKYRAzakaFGQXQJIkvcuhCAXUAHZW7ESjqxGapkGSJDhtTpx2n8bmXZshyzJynDkYM2wMTIbezx7gglgDs8D9SI9tL2WsPOtzgl2KQa7P0KYMOdOdGMvuBCUE/spB/cJj7KnFZDBh0axFKCsogyIrMBvNoZsiKygrKMOiWYv6DBMUZ6FZChlGvwvsTlAiYaCgfuEx9tTjsDqwYMYCOG1O+Pw+uNvd8Pl9cNqcWDBjARxWh94lUiehWQpfI2cnKKEM6JBHW1sbTp06hcLCwi7b9+3bh/PPPz8qhVFi4DH29FDXVAd3uxuqpsJqssLtd8PT7kF9Uz3ys/P1Lo86O9OlsLVVsztBCaXfgWLTpk1YuXIlcnJyoGkannzySUyfPh0A8PWvfx27d++OepGk3+JX0TjGHk9cJGxgDtcdhkE2YHzJeMwcOxPbP9uOimMVOFR3qN+BorcFsQY6V9HbnMG5nGsOQQ/h/hxh/7wWAZw3FTMsnwDuPT0eTsTPgFJfv3+lfPDBB7Fr1y7s2bMHTz/9NL75zW/iT3/6EwDEZTGcRx99FCNGjIDFYsH06dPx0UfhreZHA8Nj7Olh5JCRuHb6tbhy6pXItGfiqqlX4drp12LkkJF6l0a9kSTAlt3xJ1GC6Heg8Pv9GDJkCABgypQp+OCDD7Bx40b86Ec/ivnU//PPP49Vq1ZhzZo12L17NyZOnIi5c+eivr4+pu+b7niMPfXlZ+ejrLAs9P9hSZJQVljGwx1EFLZ+B4q8vDz885//DN3Pzs7Gli1b8Pnnn3fZHgsPP/wwli5diiVLlmDs2LF4/PHHYbPZ8NRTT8X0fanrMXaTYoKqqaFj7ERERGEHipaWFgDA73//e+Tl5XV5zGQy4bnnnsP7778f3eo68fl82LVrF8rLy0PbZFlGeXk5duzY0etzvF4vXC5XlxsNTPAY+5SSKbjlilswuWQyFFnBobpDepdGREQJIOyhzEsvvRSbN2/GsGHD+txn5syZUSmqN42NjVBVNXS4JWjIkCGoqKjo9Tnr1q3D2rVrY1ZTOhk5ZCRynDkoLSiFJEm4aupVKC0o5eEOCks4Vy7tbaB2IAayYFS0DWSYNBHen8OcFImwOxQXXHABpk+f3uPLe8+ePbjqqquiXlg0rF69Gs3NzaHbsWPH9C4pafEYOxERnU3YgeLpp5/G4sWLcckll2D79u04cOAAFi5ciClTpkBRYn8edE5ODhRFQV1dXZftdXV1GDp0aK/PMZvNcDqdXW5EREQUff0ayly7di1WrVqFOXPmYNy4cWhpacGOHTvw2muvxaq+EJPJhClTpuBvf/tbaJumafjb3/6Giy++OObvT0RERH0Le4airq4OP/3pT/Hkk09i7NixqKiowOLFizFtWvwuBLRq1SrcfPPNmDp1KqZNm4ZHHnkEHo8HS5YsiVsNRERE1FPYgWLkyJE477zz8Je//AVXX301Nm/ejBtuuAFHjx7FXXfdFcsaQ2644QY0NDTg/vvvx4kTJzBp0iRs3ry5x6BmuojWEBtRqovloKbeA5jRxGFOikTYgeKpp57C1772tdD9efPm4b333sM111yDw4cP49FHH41Jgd0tX74cy5cvj8t7ERERUXjCnqHoHCaCJk+ejA8//BDvvvtuVIsiIiKi5BLx5SFHjBiBDz9k652IiCidSSIeV/RKEC6XC5mZmWh++btw2s16l0MxwCuLppZ4zgn1NgeQSvMR8cSZitTR6mnBTeWlaG5uPufSCxF3KIiIiIgYKIiIiChiDBREREQUsbBPG6UEoqmAqwbQtL73kWXAWQDIsV8WnfQhBQIYfKQCshrocx9NMeBk8RgIA/+vTpTMNCFQ3dwKjzcAu9mAwkwb5DPXVkoU/FcmGbWeBKo+AFRf3/soJmDMHCAjr+99KKk5649h/BvPwOBt63OfgNmK3dcvQ3PByDhWRkTRVNngwtYDtah3e6FqGhRZRl6GGeVl+SjNTZxrVPGQRzKy5wDWzI7/bbL3vAEdj9tz9KuRYq55aDE8g4cAEtDuyOpxgwR4Bg9B89BivUslogGqbHDhhT1HUOtqg9kgw2ExwmyQUetqwwt7jqCywaV3iSEMFMlIkoH88R1/ChWQpC9uQu36OKUuWcbhC+dASDJk1d/l74Ec8ENIHY9D5t8DomSkCYGtB2rhDahwWowwKjJkSYJRkeG0GOENqNh6oBZagqz+wH9pklVWEWAfDPi9Xbf7vR3bs4r0qYviqr5kAlxDh8PU5u6y3dTugWvocNSXTNCpMiKKVHVzK+rdXthMBkjd5iUkSYLVZEC924vq5ladKuyKMxTJKtiFqNoGaAFANnT8KUnsTqSTM12KCa/9FnLAB81gguz3QUhS0nUn4n2xOy6+RInO4w2cmZnofbjeIEto0zR4vH0PZsdT8vxrQz1171KwO5GWuncp2J0gSg12swGKLEPVej+kEdAEFFmG3ZwYvQEGimQWmpWQgICX3Yl01WmWwtjuScruBBH1VJhpQ16GGa2+ALpfJUMIgTZfAHkZZhRm2nSqsCv+i5Psgl0KXxu7E2ks2KUwe1zsThClCFmSUF6WD7NBgavdD5+qQRMCPlWDq90Ps0FBeVl+wqxHwUCR7IJdCnMGuxPp7EyXoi1zMLsTRCmkNNeJhZOKke+0whfQ0NLuhy+gId9pxcJJxQm1DkViHHihyGQNBywOwJqldyWko/rSiWgdlAt3boHepZxTvAcww8Eri1KiKs11YnSOgytl0hmxXC5bkgBbdmT1UfKTJLjzCvWugohiQJYkFA2y613GWTFQxAuXyyYiohTGQBEvweWy3Q1fLI/dmc/D5bKJiChpcXIrXrhcNhERpTB2KOIpeIqnuxHovBCJ3wtk5PCUz356rXxGTF8/EQcHu4v1ZxAten+WXBWTKPYYKOIp2stlx3LQk4iIqB8YKOKte5ciku4EBz2JiChB8IB9vEVzuezgoCfQMejZ/QZw0JOIiOKCHQo9BLsUzbVAZv7AZyc6H0IRaschlCAtwEHPftD7GP9AJULd3ec49K4p3HkJLmRFFF38ptFDNJfL7n7F0SBeeZSIiOKIHQq9RGu57GgPehIREQ0Av230ElwuOxprsXfvUrA7QUREccZAkQqiOehJREQ0ADzkkSqiNehJ1E96D2GGgwOYscWFwwhghyJ1RHPQk4iIqJ/YoUgl0Rr0JCIi6icGilQSHPQkIiKKM/bFiYiIKGLsUFDSiNaVNZNhiJAGjgOY4RnoiqIcwKS+sENBREREEWOgICIioogxUBAREVHEOENBKY8zE0QDn33gzASFix0KIiIiihgDBREREUWMgYIogQkhsM/XBCGE3qUQEZ0VAwVRAvvUdxo/PP0pPvWd1rsUIqKz4lAmpZRUG8Dc4W1Eg+bFTm8jJpm5rDoRJS4GCqIEogmBd9pq0SYCAID/ba+HT6j4oL0eQxQLAMAqGXCFNR+yJOlZKhFRFwwURAnEKzT82XMYRwIeBIQGWZKQK1tQrbbi4ebPYZBkFBvsmGUZAquk6F0uEVEIZyiIEohVVrA+ezJmmnNhlGTYJQPssgF2yQCjJGOmORfrs6fAKjNMEFFiYaAgSjA5igVrsiYgT7GgVag4pXrRKlTkKRY8kDUBOYpZ7xKJiHpgoCBKQAf9LTil+RAQGqyygoDQcErz4WDArXdpRES9YqAgSkC7fKdggIT5tiI8kXMR/s02DAZI+If3pN6lERH1ikOZRAloqnkwig12zDTnQpIk3Jk5FhdbcpF75kwPIootTQhUN7fC4w3AbjagMNPGM6vOgYGCKAGdZ3TiPKMzdF+SJFxiydOxIqL0UdngwtYDtah3e6FqGhRZRl6GGeVl+SjNdZ77BdIUAwUljVRbtIoonha4H+lyn1cR7V1lgwsv7DkCb0CFzWSAIitQNYFaVxte2HMECycVM1T0gTMURERE6DjMsfVALbwBFU6LEUZFhixJMCoynBYjvAEVWw/UQuO1dXrFQEFERASgurkV9W4vbCYDpG7zEpIkwWoyoN7tRXVzq04VJjYGCiIiIgAeb+DMzETvw5cGWYKqafB4A3GuLDkwUBAREQGwmw1QZBmq1vshjYAmoMgy7GaOH/aGgYKIiAhAYaYNeRlmtPoCEN3mJIQQaPMFkJdhRmGmTacKExsDBREREQBZklBelg+zQYGr3Q+fqkETAj5Vg6vdD7NBQXkZr/TbFwYKIiKiM0pznVg4qRj5Tit8AQ0t7X74AhrynVaeMnoOPBBERETUSWmuE6NzHFwps58YKIhSjaYCrhpA0/reR5YBZwHAy6BTEovl8tiyJKFokD0qr5UuGCiIUk3rSaDqA0D19b2PYgLGzAEyuJw3JScuj514OENBlGrsOYA1s+N/m+w9b0DH4/Yc/WokikBweexaVxvMBhkOixFmgxxaHruywaV3iWmJgYIo1UgykD++40+hApL0xU2oXR8nSjJcHjtx8ZAHUSrKKgLsgwF3I9B5ER6/F8jI6XicUk9/5meSVH+Wx+YMRHwxUBClomAXomoboAUA2dDxpySxO5HK+jE/s0B+pMdDyXAF0i+Wx+59oNggS2jj8ti64L8qRKkq2KXwezvu+70d99mdSF1pMD/D5bETFwMFUaoKzUpIQMDL7kQ6SIP5GS6PnbiS928VEZ1bsEvha2N3Il1070wFpUiHistjJy4GCqJUFvyN1JyR9L+ZUpg6d6a0M3MEKTY/w+WxExMPMhGluqzhgMUBWLP0roTipftZPil4dg+Xx048DBREqU6SAFu23lVQPHU+yyeF52e4PHZiSa2/XURE1IHzMxRnDBRERKmI8zMUZzzkQUSUqjg/Q3HEQEFElKo4P0NxxEBBRKQTTQD7Wuw45Tci2+jH+Q4PZJ6kQEmKgYKISAcfnsrEY4eH4aDHCr+QYZQ0jLa34dsjjmNGdrPe5RH1G6d0iIji7MNTmbj38xJ87rbDrqjIM3lhV1R87rbj3s9L8OGpTL1LJOq3pOhQHD58GD/+8Y/x7rvv4sSJEygoKMBNN92EH/zgBzCZTHqXR0QUNk0Ajx0eBreqYKjJi+A6TBZFw1DZixM+Mx47PAwXZTXH/fDHAvcjPbYN5Aqk0XodSi5JESgqKiqgaRo2btyIkpIS7N27F0uXLoXH48H69ev1Lo+IKGz7Wuw46LEiy+BH90UdJQkYZPDjoMeKfS12jHd69CmSaACSIlDMmzcP8+bNC90fNWoU9u/fj8cee4yBgoiSyim/EX4hwyT7e33cLGtoDhhxym+Mc2VEkUnaGYrm5mZkZ/N0KCJKLtlGP4ySBp/W+z+/Xq1jQDPb2HvgIEpUSdGh6K6qqgobNmw4Z3fC6/XC6/3iEr4ulyvWpRERndX5Dg9G29vwuduOobK3y2EPIYCmgBFfyvDgfEdiHO7obR4iWq/DuYrUomuH4p577oEkSWe9VVRUdHlOdXU15s2bh69+9atYunTpWV9/3bp1yMzMDN2KiriWPRHpS5aAb484jgxFxQmfGW2qDE0AbaqMEz4zMhQV3x5xnOtRUNKRhBBCrzdvaGjAyZMnz7rPqFGjQmdy1NTUYPbs2bjooovwzDPPQJbPnod661AUFRWh+eXvwmk3R/4DEBENENehYIciGbR6WnBTeSmam5vhdDrPuq+uhzxyc3ORm5sb1r7V1dX48pe/jClTpuDpp58+Z5gAALPZDLOZwYGI4kxTAVcNoGl97jJDlnHRxFPY53FypUxKCUkxQ1FdXY3Zs2ejuLgY69evR0NDQ+ixoUOH6lgZEVEvWk8CVR8Aqq/vfRQT5DFzMN6pxK8uohhKikCxZcsWVFVVoaqqCsOGDevymI5HbIiIemfPAayZgLsBMNl7Pu7zdDxuz4l/bQmk+6AmD4Ekt6Q4bXTx4sUQQvR6IyJKOJIM5I/v+FOoHStWBW9C7fo4UYrg32YioljIKgLsgwG/t+t2v7djexbPOqPUwkBBRBQLoS6EBGiBjm1aoOM+uxOUgvg3mogoVrp3KdidoBTGQEFEFCuduxQBL7sTlNL4t5qIKJaCXQpfG7sTlNIYKIiIYinYpTBnsDtBKS0p1qEgIkpqWcMBiwOwZuldCVHMMFAQEcWaJAG2bL2rSHi8ImlyY++NiIiIIsZAQURERBFjoCAiIqKIMVAQERFRxBgoiIiIKGIMFERERBQxBgoiIiKKGAMFERERRYyBgoiIiCLGQEFEREQRY6AgIiKiiDFQEBERUcQYKIiIiChiDBREREQUMQYKIiIiiphB7wLShqYCrhpA0/reR5YBZwEgK/Gri4iIKAoYKOKl9SRQ9QGg+vreRzEBY+YAGXnxq4uIiCgKGCjixZ4DWDMBdwNgsvd83OfpeNyeE//aiIgoqWhCoLq5FR5vAHazAYWZNsiSpGtNDBTxIslA/nigahsgVEDu9NFrgS8elzjWQkREfatscGHrgVrUu71QNQ2KLCMvw4zysnyU5jp1q4vfXvGUVQTYBwN+b9ftfm/H9qwifeoiIqKkUNngwgt7jqDW1QazQYbDYoTZIKPW1YYX9hxBZYNLt9oYKOIp1IWQOroSwJnuhMTuBBERnZUmBLYeqIU3oMJpMcKoyJAlCUZFhtNihDegYuuBWmhC6FIfD3nEW7BL4W4EzIaO7kRGDrsTRDQwPIMsbVQ3t6Le7YXNZIDUbV5CkiRYTQbUu72obm5FYaYt7jMWDBTx1nmWIuBld4KIIpPiZ5AtcD/SY9tLGSvjXkci8HgDZ2Ymeg+GBllCm6bhQL0Lr+87HvcZC36L6SHYpfC1cXaCiCITPIMM6DiDrPsN4BlkKcJuNkCRZaha74c0ApqAJoDth+p1mbFgoNBDsEthzmB3gogi0/kMMaF2dD2DN6HyDLIUUphpQ16GGa2+AES3OQkhBFq9fgghENCELjMW/Buml6zhwJfmdvxJRBQJnkGWFmRJQnlZPswGBa52P3yqBk0I+FQNrnY/DHJHgLCHMWMRC5yh0IskAbZsvauILQ6LEcVH59ksLdCxzg3PIEtJpblOLJxUHFqHou3MjES+04qyXCfeP1gHRe59+DI4Y+HxBmJSGwMFxU6KD4sRJZQUPYMsXQcwz6Y014nROY4eZ3FUN7di+6EGqJqArPQMFQFNQJFl2M2x+epnbKXY4bAYUfx0XueGZ5ClPFmSUDTIjjFDMlE0yA5Zks45Y9HmCyAvw4zCTFtsaorJqxIBHBYjijeeQZbWzjVjYTYoKC/Lj9l6FDzkQbHVvQ0blCLtWKKEEgzp7S0DC+uce0p6Z5uxiPU6FAwUFFscFiOKr6zhgMUBWLP6/1zOPaWEvmYsuFImJb8UHRYjSkiRnEEWnHtyN3wx59SZz8O5pyQRnLGI63vG9d0oPXFYjCg5cO4p6jQhcKzJg4q6Zhxr8uh24a54YIeC4iPYpWiuBTLz2Z0gSlSce4qaygZXaJYhntfU0AtjJsUHlxsnSg6dO4ramQWQOPfUb5UNLryw54gu19TQCzsUFD+RDIsRUfxw7ikimhDYeqAW3oAKp8UYWgZbViQ4LUa42v3YeqAWo3McMR+UjCdGTYqf4LBYCv0fiCglce4pItXNrah3e2HT6ZoaeuHfDiIi6omLZA2Yxxs4MzPR9zU11BheU0MvDBRERNQT554GzG42QJFlqFrvZ3TE+poaekmtn4aIiKKHc08DErymRq2rrcsMBfDFNTXyndaYXVNDLwwURETUu0gWyRqAVLmyaPCaGi/sOQJXux9WkwEGWUJA6wgTsb6mhl7YwyIiIoqy4DU18p1W+AIaWtr98AU05DutWDipOCXXoWCHgoiIKAb0uqaGXhgoiIiIYkSPa2rohYGCiIgSwgL3Iz22pcpcRTrgDAURERFFjIGCiIiIIsZAQURERBFjoCAiIqKIcSiTiChZaCrgqgE0re99ZBlwFgCyEr+6YoiDmsmDgYKIKFm0ngSqPgBUX9/7KCZgzBwgIy9+dRGBgYKIKHnYcwBrJuBuAEy9rG3g83Q8bs+Jf22U9hgoiPSQhq1rioLgFUCrtgFCBeRO/4RrgS8e55VBSQcMFER6YOuaBiqrCLAPBtyNQOfLX/u9QEZOx+NEOmCgINIDW9c0UJ27FFqgo0uhBTquDJom3Ynug5oc0kwMqf83jygRdW5NC7XjyyB4Eypb13R2wS6F39tx3+/tuM/uRELQhMCxJg8q6ppxrMkDTQi9S4oLdiiI9MLWNQ1U5y5FwJtW3YlEV9ngwtYDtah3e6FqGhRZRl6GGeVl+Sl5yfLO+LePSC+hLoTU0bIG0q51TREIBlJfG7sTCaKywYUX9hxBrasNZoMMh8UIs0FGrasNL+w5gsoGl94lxhQ7FER66t6lYHeCwhUMpO0taRdAE3FmQhMCWw/UwhtQ4bQYIUkSAEBWJDgtRrja/dh6oBajcxyQzzyWatLnbyBRIurcpWDrmvorazjwpbkdf5KuqptbUe/2wmYyhMJEkCRJsJoMqHd7Ud3cqlOFscd/tYj0xtY1DZQkAbbsjj9JVx5v4MzMRO//LQyyBFXT4PEG4lxZ/DBQEOkt2KUwZ7A7QZSk7GYDFFmGqvV+RkdAE1BkGXZz6k4apO5PRpRMsoYDFgdgzdK7EiIagMJMG/IyzKh1tXWZoQAAIQTafAHkO60ozLTpWGVs8VchokTA1jVRUpMlCeVl+TAbFLja/fCpGjQh4FM1uNr9MBsUlJflp+xAJsBAQUREFBWluU4snFSMfKcVvoCGlnY/fAEN+U4rFk4qTvl1KHjIg4iIKEpKc50YneNAdXMrPN4A7GYDCjNtKd2ZCGKgoNTDK3kSkY5kSULRoF6u0ZPiGCgo9fBKnkREccdAQamHV/IkIoo7DmVS6uGVPImI4o7/olJq6n555yBe5pmIKCYYKCg18UqeRERxxX9VKXV171KwO0FEFDNJFyi8Xi8mTZoESZKwZ88evcuhRMYreRIRxU3S/ct69913o6CgQO8yKFnwSp5ERHGRVIHirbfewjvvvIP169frXQolC17Jk4goLpJmHYq6ujosXboUL7/8Mmy28K7W5vV64fV+MeXvcrliVR4lMl7Jk4go5pLi1zUhBBYvXozbb78dU6dODft569atQ2ZmZuhWVMR2d1rilTyJiGJO10Bxzz33QJKks94qKiqwYcMGtLS0YPXq1f16/dWrV6O5uTl0O3bsWIx+EiIiovSm6yGP733ve1i8ePFZ9xk1ahTeffdd7NixA2azuctjU6dOxY033ohnn3221+eazeYezyEiIqLo0zVQ5ObmIjc395z7/fKXv8SDDz4Yul9TU4O5c+fi+eefx/Tp02NZIhEREYUhKYYyhw8f3uV+RkYGAGD06NEYNmyYHiURERFRJ0kxlElERESJLSk6FN2NGDECQgi9yyAiIqIz2KEgIiKiiDFQEBERUcQYKIiIiChiDBREREQUMQYKIiIiilhSnuVBRNSDpgKuGkDT+t5HlgFnASAr8auLKE0wUBBRamg9CVR9AKi+vvdRTMCYOUBGXvzqSlcMeGmHgYKIUoM9B7BmAu4GwGTv+bjP0/G4PSf+taUjBry0wxkKIkoNkgzkj+/4U6gdl6sP3oTa9XGKvWDAAzoCXvcbwICXYvj/LCJKHVlFgH0w4Pd23e73dmzPKtKnrnTEgJd2+F+SiFJH6EtKArRAxzYt0HGfX17xx4CXVvj/LiJKLd2/xPjlpR8GvLTC/5pElFo6f4kFvPzy0hsDXtrgWR5646lVRNEX/BJrrgUy8/nlpadgwKvaxoCX4hgo9MZTq4iiL/gl1t7CL69EwICXFhgo9MZz54liI2s4YHEA1iy9KyEGvLTAQKG3zu1AoQJyp/8kWoCnVhENlCQBtmy9q6AgBryUx2+pRMBTq4go1QUDniTpXQnFCANFIuCpVURElOR4yCNRBLsU7kbAbOjoTmTkJE93gmerEBGlNQaKRJHsp1bxbBUiorTGQJFIkvnUKp6tEh52cogoRTFQJJJkPrWKZ6uEh50cIkpRDBSJJplPreo+BxKUbPMgscRODhGlqDT/dTEBJfOpVTxb5dx4SWciSlH8V4uiixcCOjeuO0JEKYiBgqKLV3o8N3ZyiCgF8V8uir7gb+C+Nv7G3Rd2cogoxTBQUPQFfwM3Z/A37r6wk0NEKYZneVBsJPPZKvGSzOuOEBF1w0BBscErPZ5bMq87QqSjBe5Hutx/KWOlLnVQVwwURHpiJ4eIUgQDBZGe2MkhohTBHisRERFFjIGCiIiIIsZAQURERBFjoCAiIqKIMVAQERFRxNLqLA8hBADA1eo9x55ERJQsWqUWvUtIWa2ejs82+P15NpIIZ68Ucfz4cRQVcTVCIiKi/jh27BiGDRt21n3SKlBomoaamho4HA5IkqR3OWFzuVwoKirCsWPH4HQ69S4nLfAzjy9+3vHFzzv+kvUzF0KgpaUFBQUFkOWzT0mk1SEPWZbPmbASmdPpTKq/iKmAn3l88fOOL37e8ZeMn3lmZmZY+3Eok4iIiCLGQEFEREQRY6BIAmazGWvWrIHZbNa7lLTBzzy++HnHFz/v+EuHzzythjKJiIgoNtihICIioogxUBAREVHEGCiIiIgoYgwUREREFDEGiiTm9XoxadIkSJKEPXv26F1OSjp8+DC++c1vYuTIkbBarRg9ejTWrFkDn8+nd2kp5dFHH8WIESNgsVgwffp0fPTRR3qXlJLWrVuHCy+8EA6HA3l5eZg/fz7279+vd1lp46GHHoIkSVi5cqXepcQEA0USu/vuu1FQUKB3GSmtoqICmqZh48aN2LdvH37xi1/g8ccfx7333qt3aSnj+eefx6pVq7BmzRrs3r0bEydOxNy5c1FfX693aSnn/fffx7Jly7Bz505s2bIFfr8fV1xxBTwej96lpbyPP/4YGzduxIQJE/QuJXYEJaU333xTjBkzRuzbt08AEJ988oneJaWNn//852LkyJF6l5Eypk2bJpYtWxa6r6qqKCgoEOvWrdOxqvRQX18vAIj3339f71JSWktLiygtLRVbtmwRs2bNEitWrNC7pJhghyIJ1dXVYenSpfj9738Pm82mdzlpp7m5GdnZ2XqXkRJ8Ph927dqF8vLy0DZZllFeXo4dO3boWFl6aG5uBgD+fY6xZcuW4eqrr+7y9zwVpdXFwVKBEAKLFy/G7bffjqlTp+Lw4cN6l5RWqqqqsGHDBqxfv17vUlJCY2MjVFXFkCFDumwfMmQIKioqdKoqPWiahpUrV2LmzJkYN26c3uWkrD//+c/YvXs3Pv74Y71LiTl2KBLEPffcA0mSznqrqKjAhg0b0NLSgtWrV+tdclIL9/PurLq6GvPmzcNXv/pVLF26VKfKiaJj2bJl2Lt3L/785z/rXUrKOnbsGFasWIE//vGPsFgsepcTc1x6O0E0NDTg5MmTZ91n1KhRWLhwIV577TVIkhTarqoqFEXBjTfeiGeffTbWpaaEcD9vk8kEAKipqcHs2bNx0UUX4ZlnnoEsM4tHg8/ng81mw6ZNmzB//vzQ9ptvvhlNTU145ZVX9CsuhS1fvhyvvPIKPvjgA4wcOVLvclLWyy+/jAULFkBRlNA2VVUhSRJkWYbX6+3yWLJjoEgyR48ehcvlCt2vqanB3LlzsWnTJkyfPh3Dhg3TsbrUVF1djS9/+cuYMmUK/vCHP6TUPwCJYPr06Zg2bRo2bNgAoKMVP3z4cCxfvhz33HOPztWlFiEEvvOd7+Cll17Ctm3bUFpaqndJKa2lpQVHjhzpsm3JkiUYM2YMvv/976fcoSbOUCSZ4cOHd7mfkZEBABg9ejTDRAxUV1dj9uzZKC4uxvr169HQ0BB6bOjQoTpWljpWrVqFm2++GVOnTsW0adPwyCOPwOPxYMmSJXqXlnKWLVuGP/3pT3jllVfgcDhw4sQJAEBmZiasVqvO1aUeh8PRIzTY7XYMHjw45cIEwEBBdFZbtmxBVVUVqqqqegQ2Nvei44YbbkBDQwPuv/9+nDhxApMmTcLmzZt7DGpS5B577DEAwOzZs7tsf/rpp7F48eL4F0QphYc8iIiIKGKcLCMiIqKIMVAQERFRxBgoiIiIKGIMFERERBQxBgoiIiKKGAMFERERRYyBgoiIiCLGQEFEREQRY6AgIiKiiDFQEFFcPffcc7BaraitrQ1tW7JkCSZMmIDm5mYdKyOiSHDpbSKKKyEEJk2ahMsuuwwbNmzAmjVr8NRTT2Hnzp0oLCzUuzwiGiBeHIyI4kqSJPzkJz/B9ddfj6FDh2LDhg343//931CYWLBgAbZt24bLL78cmzZt0rlaIgoXOxREpIvJkydj3759eOeddzBr1qzQ9m3btqGlpQXPPvssAwVREuEMBRHF3ebNm1FRUQFVVXtcpnz27NlwOBw6VUZEA8VAQURxtXv3bixcuBC//e1vcfnll+OHP/yh3iURURRwhoKI4ubw4cO4+uqrce+992LRokUYNWoULr74YuzevRuTJ0/WuzwiigA7FEQUF6dOncK8efPwb//2b7jnnnsAANOnT8eVV16Je++9V+fqiChS7FAQUVxkZ2ejoqKix/Y33nhDh2qIKNp4lgcRJZTy8nJ8+umn8Hg8yM7Oxl/+8hdcfPHFepdFROfAQEFEREQR4wwFERERRYyBgoiIiCLGQEFEREQRY6AgIiKiiDFQEBERUcQYKIiIiChiDBREREQUMQYKIiIiihgDBREREUWMgYKIiIgixkBBREREEWOgICIiooj9f79lOIhuLOpYAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Decision Trees\n",
        "\n",
        "Decision trees are both a learning model in their own right and an important constituent model for the ensemble methods in subsequent tasks. The model consists of a recursive sequence of binary tests, typically of inequalities on single feature values. These effectively partition the feature space into separate regions, each of which is then assigned the class that occurs most often among training samples within it.(Equivalently, those training samples *vote* on the outcome. Once again, the `utils.vote` function may be useful here.)\n",
        "\n",
        "Trees can be trained by a greedy brute force search for the split that minimises some chosen loss. Various losses are possible, but for simplicity here (and compatibility with Task 4 later) we will use a **weighted misclassification error** throughout:\n",
        "\n",
        "$$\n",
        "L(\\mathbf{y}, \\hat{\\mathbf{y}}, \\mathbf{w}) = \\sum_i w_i \\mathbb{1}(y_i \\neq \\hat{y}_i)\n",
        "$$\n",
        "\n",
        "Trees are a naturally recursive data structure that almost cry out for a class-based implementation, but here we limit ourselves to using dicts to avoid adding syntactical distractions for students who may not be very familiar with Python classes."
      ],
      "metadata": {
        "id": "FMWP8eI3Ba46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Calculate a weighted misclassification error\n",
        "\n",
        "Implement the `misclassification` function in the code cell below. Note that if `weights` is not provided you should default it to $\\frac{1}{n}$.\n"
      ],
      "metadata": {
        "id": "rpZPvjqBBmqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def misclassification ( y, cls, weights=None ):\n",
        "    \"\"\"\n",
        "    Calculate (optionally-weighted) misclassification error for\n",
        "    a given set of labels if assigned the given class.\n",
        "\n",
        "    # Arguments\n",
        "        y: a set of class labels\n",
        "        cls: a candidate classification for the set\n",
        "        weights: optional weights vector specifying relative\n",
        "            importance of the samples labelled by y\n",
        "\n",
        "    # Returns\n",
        "        err: the misclassification error of the candidate labels\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: implement weighted misclassification metric\n",
        "    # Create default weights - make everything equally probable\n",
        "    if weights is None:\n",
        "      weights = 1/len(y)\n",
        "    # return number of weighted misclassifications\n",
        "    return mp.sum(weights * (y != cls))"
      ],
      "metadata": {
        "id": "2rTVOKdyLFMB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Split a dataset to reduce misclassification\n",
        "\n",
        "Implement the body of the `decision_node_split` function in the cell below. This function is quite involved and there are some finicky details, so read the docstring carefully.\n",
        "\n",
        "Some things to note:\n",
        "* It's a **brute force** search. Yes, this will scale badly and may also offend your coding sensibilities; just go with it.\n",
        "* The total loss after a split must be scaled according to the samples in each child node. You should be able to use the `weights` argument to help with this.\n",
        "* You need to decide on some tie-break policy for when multiple splits produce the same loss improvement.\n",
        "* The return values from this function are effectively **instructions** on how to split. You don't return the actual child node data."
      ],
      "metadata": {
        "id": "gb13HFsdB1Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_node_split ( X, y, cls=None, weights=None, min_size=3 ):\n",
        "    \"\"\"\n",
        "    Find (by brute force) a split point that best improves the weighted\n",
        "    misclassification error rate compared to the original one (or not, if\n",
        "    there is no improvement possible).\n",
        "\n",
        "    Features are assumed to be numeric and the test condition is\n",
        "    greater-or-equal.\n",
        "\n",
        "    # Arguments:\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and columns are features.\n",
        "        y: vector of class labels corresponding to the samples,\n",
        "            must be same length as number of rows in X\n",
        "        cls: class label currently assigned to the whole set\n",
        "            (if not specified we use the most common class in y, or\n",
        "            the lowest such if 2 or more classes occur equally)\n",
        "        weights: optional weights vector specifying relevant importance\n",
        "            of the samples\n",
        "        min_size: don't create child nodes smaller than this\n",
        "\n",
        "    # Returns:\n",
        "        feature: index of the feature to test (or None, if no split)\n",
        "        thresh: value of the feature to test (or None, if no split)\n",
        "        c0: class assigned to the set with feature < thresh (or None, if no split)\n",
        "        c1: class assigned to the set with feature >= thresh (or None, if no split)\n",
        "    \"\"\"\n",
        "    assert(X.shape[0] == len(y))\n",
        "    feature, thresh, c0, c1 = None, None, None, None\n",
        "\n",
        "    # TODO: implement this\n",
        "    num_data = len(y)\n",
        "\n",
        "    # if there aren't enough samples no split will be legal\n",
        "    if num_data < min_size * 2:\n",
        "      return feature, thresh, c0, c1\n",
        "\n",
        "    # default to the most common class\n",
        "    if cls is None:\n",
        "      cls = utils.vote(y)\n",
        "\n",
        "    # default weights\n",
        "    if weights is None:\n",
        "      weights = np.ones(num_data) / num_data\n",
        "\n",
        "    # parent node loss is what we're aiming to beat\n",
        "    best_loss = misclassification(y, cls=cls, weights=weights)\n",
        "\n",
        "    # if it's already perfect, stop\n",
        "    if best_loss == 0:\n",
        "      return feature, thresh, c0, c1\n",
        "\n",
        "    # keep track of our best candidate to date\n",
        "    best_feat, best_thresh = None, None\n",
        "    best_c0, best_c1 = None, None\n",
        "\n",
        "\n",
        "    # slog through all features and threshold values\n",
        "    assert len(X.shape) == 2\n",
        "    num_features = X.shape[1]\n",
        "    # At each node, the algorithm chooses:\n",
        "    # - which feature to split on, and\n",
        "    # - what threshold value to use for that feature.\n",
        "    # | <... features ...> |\n",
        "    # | ... f .............|\n",
        "    # | ... f .............|\n",
        "    # | ...................|\n",
        "    # | ...................|\n",
        "    # | ... f .............|\n",
        "    for feat in range(num_features):\n",
        "        # unique values in i-th feature column\n",
        "        for thresh in np.unique(X[:,feat]):\n",
        "            # Decide how to split in the dumbest way\n",
        "            # use boolean index sets - either IS this feature or is NOT\n",
        "            set1 = X[:,feat] >= thresh\n",
        "            set0 = ~set1\n",
        "\n",
        "            # disallow splits producing too small children\n",
        "            if (np.sum(set0) < min_size) or (np.sum(set1) < min_size):\n",
        "                continue\n",
        "\n",
        "            # select the labels and reqpestive matching weights\n",
        "            y0, y1= y[set0], y[set1]\n",
        "            w0, w1 = weights[set0], weights[set1]\n",
        "\n",
        "            # any label that occurs in the node is a candidate to assign\n",
        "            cc0, cc1 = np.unique(y0), np.unique(y1)\n",
        "\n",
        "            # calculate the loss associated with each candidate label in each child\n",
        "            # note that because we pass the weights, these losses will scale for node size\n",
        "            # MaaTt we want to calculate loss for ALL labels in set again ALL candidate laves\n",
        "            # - so that we can calculate argmin later (we need to take weights intio account)\n",
        "            losses0 = [misclassification(y0, cls=cc, weights=w0) for cc in cc0]\n",
        "            losses1 = [misclassification(y1, cls=cc, weights=w1) for cc in cc1]\n",
        "\n",
        "            # determine best labels to assign to each node\n",
        "            print(f\"np.argmin(losses0): {np.argmin(losses0)}\")\n",
        "            c0, c1 = cc0[np.argmin(losses0)], cc1[np.argmin(losses1)]\n",
        "\n",
        "            # and the corresponding weighted losses\n",
        "            loss = np.min(losses0) + np.min(losses1)\n",
        "\n",
        "            # special case: if split is perfect we can stop searching right now\n",
        "            if loss == 0:\n",
        "                return feat, thresh, c0, c1\n",
        "\n",
        "            # if loss is better than we've seen so far, update\n",
        "            # note that this (along with the special case above) implicitly defines\n",
        "            # a tie-break policy: first come, first served\n",
        "            if loss < best_loss:\n",
        "                best_feat = feat\n",
        "                best_thresh = thresh\n",
        "                best_c0 = c0\n",
        "                best_c1 = c1\n",
        "                best_loss = loss\n",
        "\n",
        "    return feature, thresh, c0, c1"
      ],
      "metadata": {
        "id": "SEYLZNSzLFok"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Train a decision tree classifier\n",
        "\n",
        "Implement the body of the `decision_tree_train` function in the cell below.\n",
        "\n",
        "You should find that most of the hard work is already done by your `decision_node_split` function, and this is just responsible for managing the recursion. But doing so is also a bit finicky. Again, you should read the docstring carefully to get a handle on the returned data structure."
      ],
      "metadata": {
        "id": "O_6LsBT0RCNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_tree_train ( X, y, cls=None, weights=None, min_size=3, depth=0, max_depth=10 ):\n",
        "    \"\"\"\n",
        "    Recursively choose split points for a training dataset\n",
        "    until no further improvement occurs.\n",
        "\n",
        "    # Arguments:\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and columns are features.\n",
        "        y: vector of class labels corresponding to the samples,\n",
        "            must be same length as number of rows in X\n",
        "        cls: class label currently assigned to the whole set\n",
        "            (if not specified we use the most common class in y, or\n",
        "            the lowest such if 2 or more classes occur equally)\n",
        "        weights: optional weights vector specifying relevant importance\n",
        "            of the samples\n",
        "        min_size: don't create child nodes smaller than this\n",
        "        depth: current recursion depth\n",
        "        max_depth: maximum allowed recursion depth\n",
        "\n",
        "    # Returns:\n",
        "        tree: a dict containing (some of) the following keys:\n",
        "            'kind' : either 'leaf' or 'decision'\n",
        "            'class' : the class assigned to this node (for a leaf)\n",
        "            'feature' : index of feature on which to split (for a decision)\n",
        "            'thresh' : threshold at which to split the feature (for a decision)\n",
        "            'below' : a nested tree applicable when feature < thresh\n",
        "            'above' : a nested tree applicable when feature >= thresh\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "\n",
        "    return {\n",
        "        'kind': 'decision',\n",
        "        'feature': None,\n",
        "        'thresh': None,\n",
        "        # Not yet done, so recurse\n",
        "        'below': None,\n",
        "        'above': None,\n",
        "    }"
      ],
      "metadata": {
        "id": "wbG26t1eRWk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Make predictions from a decision tree\n",
        "\n",
        "Implement the body of the `decision_tree_predict` function in the cell below. You will need to walk the tree for each sample, testing decision nodes until you reach a leaf.\n",
        "\n",
        "You may find it helpful to define an auxiliary function to process a single sample."
      ],
      "metadata": {
        "id": "e3hGm1bWPCm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_tree_predict ( tree, X ):\n",
        "    \"\"\"\n",
        "    Predict labels for test data using a fitted decision tree.\n",
        "\n",
        "    # Arguments\n",
        "        tree: a decision tree dictionary returned by decision_tree_train\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and columns are features.\n",
        "\n",
        "    # Returns\n",
        "        y: the predicted labels\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "91gs5dXIPGKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 2\n",
        "\n",
        "Execute the code cell below to use your functions above to train and test a decision tree classifier and generate a plot.\n",
        "\n",
        "As in Task 1, try playing with different values for `NUM_SAMPLES` and `RESOLUTION` and see how this affects the results — and the running time."
      ],
      "metadata": {
        "id": "7cOF1Dyfig7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES = 50\n",
        "RESOLUTION = 100\n",
        "MIN_SIZE = 3\n",
        "\n",
        "X = df[['X1','X2']].values[:NUM_SAMPLES,:]\n",
        "y = df['Multi'].values[:NUM_SAMPLES]\n",
        "\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "ax = fig.subplots()\n",
        "\n",
        "tree = decision_tree_train ( X, y, min_size=MIN_SIZE )\n",
        "if tree is None:\n",
        "    utils.plot_unimplemented(ax, f'Decision Tree')\n",
        "else:\n",
        "    tree_cls = lambda z: decision_tree_predict ( tree, z )\n",
        "    utils.plot_classification_map(ax, tree_cls, X, y, resolution=RESOLUTION, title=f'Decision Tree')"
      ],
      "metadata": {
        "id": "o-zoeOPlLG15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Random Forests\n",
        "\n",
        "Random forests<sup><small>TM</small></sup> are an ensemble model aggregating the predictions from multiple decision trees. Diversity is introduced into the ensemble by training the trees on **bootstrap samples** from the training set, and also by restricting the subset of features used by each tree.\n",
        "\n",
        "For the exercises below, we will forgo feature subsetting (we will only be using two features anyway) and focus on the **bagging** aspect.\n"
      ],
      "metadata": {
        "id": "IhLI-HSFUjj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Train a (simplified) random forest classifier\n",
        "\n",
        "Implement the `random_forest_train` function in the code cell below.\n",
        "\n",
        "Use the `decision_tree_train` function you wrote in Task 2.3 to train the individual trees. The [choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html) method of the supplied `rng` object should help with bootstrap sampling."
      ],
      "metadata": {
        "id": "Xxl22B-kXqRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_forest_train ( X, y, k, rng, min_size=3, max_depth=10 ):\n",
        "    \"\"\"\n",
        "    Train a (simplified) random forest of decision trees.\n",
        "\n",
        "    # Arguments:\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and columns are features.\n",
        "        y: vector of binary class labels corresponding to the\n",
        "            samples, must be same length as number of rows in X\n",
        "        k: the number of trees in the forest\n",
        "        rng: an instance of numpy.random.Generator\n",
        "            from which to draw random numbers\n",
        "        min_size: don't create child nodes smaller than this\n",
        "        max_depth: maximum tree depth\n",
        "\n",
        "    # Returns:\n",
        "        forest: a list of tree dicts as returned by decision_tree_train\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "yHp-QfcBX5ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Make predictions from a (simplified) random forest classifier\n",
        "\n",
        "Implement the `random_forest_predict` function in the cell below.\n",
        "\n",
        "Use the `decision_tree_predict` function you wrote in Task 2.4 to predict from the individual trees.\n",
        "\n",
        "Once again, the `utils.vote` function may be useful here."
      ],
      "metadata": {
        "id": "VHDxYFfcTXgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_forest_predict ( forest, X ):\n",
        "    \"\"\"\n",
        "    Predict labels for test data using a fitted random\n",
        "    forest of decision trees.\n",
        "\n",
        "    # Arguments\n",
        "        forest: a list of decision tree dicts\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and columns are features.\n",
        "\n",
        "    # Returns\n",
        "        y: the predicted labels\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "lAU0JqXCTX9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 3\n",
        "\n",
        "Execute the cell below to train and test a simplified random forest classifier and produce a plot.\n",
        "\n"
      ],
      "metadata": {
        "id": "SG9VU7mwYtzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES = 50\n",
        "RESOLUTION = 100\n",
        "NUM_TREES = 10\n",
        "\n",
        "X = df[['X1','X2']].values[:NUM_SAMPLES,:]\n",
        "y = df['Multi'].values[:NUM_SAMPLES]\n",
        "\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "ax = fig.subplots()\n",
        "\n",
        "forest = random_forest_train ( X, y, NUM_TREES, rng=shared_rng )\n",
        "if forest is None:\n",
        "    utils.plot_unimplemented(ax, f'Random Forest')\n",
        "else:\n",
        "    forest_cls = lambda z: random_forest_predict ( forest, z )\n",
        "    utils.plot_classification_map(ax, forest_cls, X, y, resolution=RESOLUTION, title=f'Random Forest ({NUM_TREES} trees)')"
      ],
      "metadata": {
        "id": "B9gZpHH3YuXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: AdaBoost\n",
        "\n",
        "AdaBoost is a meta-algorithm that iteratively builds an ensemble of weak learners such that each new addition provides the best available marginal improvement in the ensemble performance. The new learner is chosen to minimise its weighted classification error on the training set, with the sample weights updated at each iteration to prioritise misclassified points. The training procedure is shown in pseudocode form below:\n",
        "\n",
        "\n",
        "* Initialise sample weights $w_i = \\frac{1}{n}, \\quad i \\in \\{1, 2, \\dots, n\\}$\n",
        "* **for** t = 1 to k **do**:\n",
        "  * fit classifier $h_t$ to minimise misclassification error with weights $w_i$\n",
        "  * set $\\epsilon =$ the weighted misclassification error of $h_t$\n",
        "  * compute prediction weight: $\\alpha_t = \\log\\big(\\frac{1-\\epsilon}{\\epsilon}\\big)$\n",
        "  * update weights: $w_i \\leftarrow w_i \\exp(\\alpha_t \\mathbb{1}(y_i \\neq h_i(x_i)))$\n",
        "  * normalise weights: $w_i = \\frac{w_i}{\\sum_j w_j}$\n",
        "\n",
        "Once the ensemble is trained, new samples are classified like this:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\mathbb{1}\\left(\\sum_t \\alpha_t h_t(\\mathbf{x}) \\ge 0\\right)\n",
        "$$\n",
        "\n",
        "Note that the training algorithm has been expressed in terms that don't require a particular binary labelling convention, but the prediction expression above assumes that the outputs of the classifiers $h_t$ are $\\{-1, 1\\}$. This is *not* the case for the decision trees implemented in Task 2, nor for the synthetic data. So you will need to convert the $h_t$ outputs appropriately within the prediction sum.\n",
        "\n",
        "AdaBoost is agnostic as to the class of weak learners used, but is commonly implemented using **decision stumps** — decision trees of depth 1 — and that is what you should do here, using the decision tree functions you implemented in Task 2."
      ],
      "metadata": {
        "id": "swxeclj-B54s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Train an AdaBoost classifier\n",
        "\n",
        "Provide an implementation body for the `adaboost_train` function defined in the cell below.\n",
        "\n",
        "Once again, read the docstring carefully, as there are some fiddly details.\n"
      ],
      "metadata": {
        "id": "KXLDZUltCAFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adaboost_train ( X, y, k, min_size=1, max_depth=1, epsilon=1e-8 ):\n",
        "    \"\"\"\n",
        "    Iteratively train a set of decision tree classifiers\n",
        "    using AdaBoost.\n",
        "\n",
        "    # Arguments:\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and columns are features.\n",
        "        y: vector of binary class labels corresponding to the\n",
        "            samples, must be same length as number of rows in X\n",
        "        k: the maximum number of weak classifiers to train\n",
        "        min_size: don't create child nodes smaller than this\n",
        "        max_depth: maximum tree depth -- by default we just\n",
        "            use decision stumps\n",
        "        epsilon: threshold below which the error is considered 0\n",
        "\n",
        "    # Returns:\n",
        "        trees: a list of tree dicts as returned by decision_tree_train\n",
        "        alphas: a vector of weights indicating how much credence to\n",
        "            given each of the decision tree predictions\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "SA_hr24lNqOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Make predictions from an AdaBoost classifier\n",
        "\n",
        "Implement the `adaboost_predict` function in the code cell below.\n",
        "\n",
        "As noted above, you will need to map the decision tree predictions from $\\{0, 1\\}$ to $\\{-1, 1\\}$."
      ],
      "metadata": {
        "id": "TAWiXdBPb1lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adaboost_predict ( trees, alphas, X ):\n",
        "    \"\"\"\n",
        "    Predict labels for test data using a fitted AdaBoost\n",
        "    ensemble of decision trees.\n",
        "\n",
        "    # Arguments\n",
        "        trees: a list of decision tree dicts\n",
        "        alphas: a vector of weights for the trees\n",
        "        X: an array of sample data, where rows are samples\n",
        "            and columns are features.\n",
        "\n",
        "    # Returns\n",
        "        y: the predicted labels\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "_rNQv3JQcGFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 4\n",
        "\n",
        "Execute the code cell below to train and test an AdaBoost classifier and plot the results.\n"
      ],
      "metadata": {
        "id": "7DBIxl_ZitVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES = 50\n",
        "RESOLUTION = 100\n",
        "NUM_TREES = 10\n",
        "\n",
        "X = df[['X1','X2']].values[:NUM_SAMPLES,:]\n",
        "y = df['Binary'].values[:NUM_SAMPLES]\n",
        "\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "ax = fig.subplots()\n",
        "\n",
        "trees, alphas = adaboost_train ( X, y, NUM_TREES )\n",
        "if forest is None:\n",
        "    utils.plot_unimplemented(ax, f'AdaBoost')\n",
        "else:\n",
        "    ada_cls = lambda z: adaboost_predict ( trees, alphas, z )\n",
        "    utils.plot_classification_map(ax, ada_cls, X, y, resolution=RESOLUTION, title=f'AdaBoost ({NUM_TREES} stumps)')"
      ],
      "metadata": {
        "id": "T-C7I1zBNr4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further exploration\n",
        "\n",
        "If you have exhausted the previous exercises, you might find it interesting to try out one or more of the following challenges. Doing so is entirely optional, but may provide some additional perspective that could be useful in the weeks ahead."
      ],
      "metadata": {
        "id": "ENAVpErdCTaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adapt your decision trees to use a different loss\n",
        "\n",
        "The decision trees for Task 2 use a weighted misclassification error in order to easily use them with AdaBoost. However, other loss functions may be preferable for some problems. Two common choices are the Gini impurity:\n",
        "\n",
        "\\begin{equation}\n",
        "\\sum_k p_k (1 - p_k)\n",
        "\\end{equation}\n",
        "\n",
        "and the entropy (or cross-entropy):\n",
        "\n",
        "\\begin{equation}\n",
        "- \\sum_k p_k \\log p_k\n",
        "\\end{equation}\n",
        "\n",
        "where $k$ ranges over all the classes present in the node and $p_k$ is the fraction of samples in the node that are of class $k$.\n",
        "\n",
        "Try modifying your implementation to use one or both of these losses and see what difference (if any) it makes to the splits chosen and the performance of the trees. (There is an implementation of `gini_impurity` in the `utils` module.)\n"
      ],
      "metadata": {
        "id": "HqJRn5FmCXNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add feature subset selection to your random forests\n",
        "\n",
        "For simplicity in Task 3 we omitted feature subsetting as a diversification mechanism in the ensemble. Try adding this and seeing if it makes much difference to your fits. You will need to adapt the data structure used for the forest to keep track of which trees are using which features so that the correct subsets can be used at test time.\n",
        "\n",
        "You may want to find some higher-dimensional data to apply your modified algorithm to, as the supplied data---having only two features---doesn't provide very much scope for variation. As mentioned last week, the [`scikit-learn` datasets](https://scikit-learn.org/stable/datasets.html) package can be a good source of data to play with.\n"
      ],
      "metadata": {
        "id": "5rf1J7K4L10r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement an ExtraTrees ensemble\n",
        "\n",
        "Bagging and feature subsetting are not the only way to build a diverse ensemble of decision trees. In the ExtraTrees or **extremely randomised trees** approach, the training algorithm for the trees is modified instead. At each split point, rather than performing a brute force search over all possible splits, a random search is used instead: some specified (but relatively small) number of purely random candidate splits are evaluated, and the best of these is chosen. This can result in a high degree of variation in the ensemble that *may* better probe the structure of the data distribution.\n",
        "\n",
        "Try implementing this fitting procedure and comparing its behaviour to that of the random forest. Is there any meaningful difference for our simple data?\n"
      ],
      "metadata": {
        "id": "m-usBkyvOAP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare the behaviour of these models in `scikit-learn`\n",
        "\n",
        "For practical applications, you will almost never need to implement classic ML models like decision trees and random forests yourself. Instead, you will typically use existing implementations such as those in the [`scikit-learn`](https://scikit-learn.org/stable/index.html) library. Because they are so widely used, these are likely to be more versatile, better optimised and better tested than your own code.\n",
        "\n",
        "`scikit-learn` supports a wide range of ML models and algorithms, including all those in the lab exercises above: [nearest neighbours](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification), [decision trees](https://scikit-learn.org/stable/modules/tree.html), [AdaBoost](https://scikit-learn.org/stable/modules/ensemble.html#adaboost) and [forests of randomised trees](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees). Models have (mostly) consistent interfaces for training and evaluation, so it is usually straightforward to swap between different models and compare their behaviour.\n",
        "\n",
        "We will see a bit more of this library in future weeks, but by all means get acquainted with it now if you have the time and inclination."
      ],
      "metadata": {
        "id": "jikCLZ3nfBLe"
      }
    }
  ]
}